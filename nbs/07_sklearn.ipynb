{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn\n",
    "\n",
    "Helper classes for Scikit Learn estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from nbdev.export import notebook2script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sklearn\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "class ScikitLearner:\n",
    "    \"\"\"Helper class to use fine tune parameter based on validation dataset performace.\n",
    "    Feel free to implement your own version.\"\"\"\n",
    "    def __init__(self, learner, X_train, X_val, y_train, y_val, predict_proba=True):\n",
    "        \"\"\"Create the class instance.\n",
    "        \n",
    "        **Parameters**\n",
    "        \n",
    "        - learner: the scikit-learn estimator\n",
    "        - X_train, X_val, y_train, y_val: train validation datasets\n",
    "        - predict_proba: whether the estimator can predict probability\n",
    "        \"\"\"\n",
    "        self.learner = learner\n",
    "        self.predict_proba = predict_proba\n",
    "        self._proba = None\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = X_train, X_val, y_train, y_val\n",
    "\n",
    "    def __getattr__(self, key):\n",
    "        return getattr(self.learner, key)\n",
    "\n",
    "    def __dir__(self):\n",
    "        return set(super().__dir__() + list(self.__dict__.keys()) +\n",
    "                   dir(self.learner))\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def valid_loss_with_params(self, loss_func=None, callbacks=tuple(), **params):\n",
    "        \"\"\"Calculate loss of the estimator on validation set, and also the values of callbacks.\n",
    "\n",
    "        **Parameters**\n",
    "        \n",
    "        - loss_func: loss function to use. By default, if estimator support to predict probability, it will be\n",
    "        log_loss, otherwise it will be accuracy score.\n",
    "        - callbacks: callbacks to also evaluate. Default is empty tuple.\n",
    "        - params: parameters to use when train the estimator.\n",
    "\n",
    "        **Returns**\n",
    "        \n",
    "        Return values will be a two element tuple.\n",
    "        - the first is values of the loss function\n",
    "        - the other is values of all callbacks\n",
    "        \"\"\"\n",
    "        return self.loss_with_params(self.X_val,\n",
    "                                     self.y_val,\n",
    "                                     loss_func=loss_func,\n",
    "                                     callbacks=callbacks,\n",
    "                                     **params)\n",
    "\n",
    "    def loss_with_params(self, X, y, loss_func=None, callbacks=tuple(), **params):\n",
    "        estimator = sklearn.clone(self.learner)\n",
    "        estimator.set_params(**params)\n",
    "        estimator.fit(self.X_train, self.y_train)\n",
    "        if self.predict_proba:\n",
    "            if self._proba is None:\n",
    "                try:                \n",
    "                    y_pred = estimator.predict_proba(self.X_val)\n",
    "                    self._proba = True\n",
    "                except:\n",
    "                    self._proba = False\n",
    "                    y_pred = estimator.predict(self.X_val)\n",
    "            elif self._proba:\n",
    "                y_pred = estimator.predict_proba(self.X_val)\n",
    "            else:\n",
    "                y_pred = estimator.predict(self.X_val)\n",
    "        else:\n",
    "            y_pred = estimator.predict(self.X_val)\n",
    "        if loss_func is None:\n",
    "            if self._proba:\n",
    "                loss_func = log_loss\n",
    "            else:\n",
    "                loss_func = lambda x,y: -accuracy_score(x,y)\n",
    "        return [loss_func(self.y_val, y_pred)], [callback(self.y_val, y_pred) for callback in callbacks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ScikitLearner.__init__)\n",
    "show_doc(ScikitLearner.valid_loss_with_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of using `ScikitLearner`\n",
    "`Digits` is a sub-class of `ScikitLearner`. Check `sample_cases` for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eptune.sample_cases import Digits\n",
    "from eptune.algorithms import eaSimpleWithExtraLog, eaMuPlusLambdaWithExtraLog, eaMuCommaLambdaWithExtraLog\n",
    "from eptune.parameter import *\n",
    "\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "from deap import algorithms\n",
    "from functools import partial\n",
    "from sklearn.metrics import *\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "params = [\n",
    "    LogFloatParameter([0.1, 1000], 'C'),\n",
    "    CategoricalParameter(['poly', 'rbf', 'sigmoid'], \"kernel\"),\n",
    "    LogFloatParameter([1e-6, 1e6], 'gamma')\n",
    "]\n",
    "\n",
    "svc_digits = Digits(SVC())\n",
    "\n",
    "\n",
    "def initParams(cls):\n",
    "    return cls({i.name: next(i) for i in cls.params})\n",
    "\n",
    "\n",
    "def evaluate(params):\n",
    "    return svc_digits.valid_loss_with_params(callbacks=(accuracy_score, ), **params)\n",
    "\n",
    "\n",
    "creator.create(\"Loss\", base.Fitness, weights=(-1.0, ))\n",
    "creator.create(\"Parameters\", dict, params=params, fitness=creator.Loss)\n",
    "toolbox.register(\"individual\", initParams, creator.Parameters)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register('evaluate', evaluate)\n",
    "from eptune.utils import ConcurrentMap\n",
    "pmap = ConcurrentMap(10)\n",
    "toolbox.register('map', pmap.map)\n",
    "\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "from eptune.crossover import cxDictUniform\n",
    "toolbox.register(\"mate\", cxDictUniform, indpb=0.5)\n",
    "\n",
    "from eptune.mutation import mutDictRand\n",
    "toolbox.register(\"mutate\", partial(mutDictRand, params=params, indpb=0.6))\n",
    "\n",
    "import numpy\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", numpy.mean, axis=0)\n",
    "stats.register(\"std\", numpy.std, axis=0)\n",
    "stats.register(\"min\", numpy.min, axis=0)\n",
    "stats.register(\"max\", numpy.max, axis=0)\n",
    "hof = tools.HallOfFame(2)\n",
    "\n",
    "\n",
    "def run():\n",
    "    return eaSimpleWithExtraLog(toolbox.population(10),\n",
    "                                toolbox,\n",
    "                                cxpb=0.6,\n",
    "                                mutpb=0.6,\n",
    "                                ngen=16,\n",
    "                                halloffame=hof,\n",
    "                                elitism=True,\n",
    "                                stats=stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time population, logbook = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = logbook.plot(['min','avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Hall of Fame with extra info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,i.extra) for i in hof.items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SVC with probability support, which is much slower than the default SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return accuracy_score(y_true, y_pred.argmax(-1))\n",
    "\n",
    "svc_digits_proba = Digits(SVC(probability=True))\n",
    "\n",
    "def evaluate_proba(params):\n",
    "    return svc_digits_proba.valid_loss_with_params(callbacks=(accuracy,), **params)\n",
    "\n",
    "from eptune.crossover import cxDictBlenderIfCan\n",
    "toolbox.register(\"mate\", cxDictBlenderIfCan, alpha=0.5, indpb=0.8, fix_invalid=True)\n",
    "toolbox.register('evaluate', evaluate_proba)\n",
    "\n",
    "# Because we are using the same HallofFame, we need to clear it before use.\n",
    "hof.clear()\n",
    "pmap.close()\n",
    "pmap = ConcurrentMap(10)\n",
    "toolbox.register('map', pmap.map)\n",
    "def run_proba():\n",
    "    return eaSimpleWithExtraLog(toolbox.population(10),\n",
    "                                toolbox,\n",
    "                                cxpb=0.6,\n",
    "                                mutpb=0.3,\n",
    "                                ngen=16,\n",
    "                                halloffame=hof,\n",
    "                                elitism=True,\n",
    "                                stats=stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time population, logbook = run_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = logbook.plot(['min','avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,i.extra) for i in hof.items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using other algorithms to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `eaMuPlusLambdaWithExtraLog`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hof.clear()\n",
    "pmap.close()\n",
    "pmap = ConcurrentMap(10)\n",
    "from eptune.crossover import cxDictBlenderIfCan\n",
    "toolbox.register(\"mate\", cxDictBlenderIfCan, alpha=0.5, indpb=0.8, fix_invalid=True)\n",
    "toolbox.register('evaluate', evaluate)\n",
    "toolbox.register('map', pmap.map)\n",
    "def run_mu_plus_lambda():\n",
    "    return eaMuPlusLambdaWithExtraLog(toolbox.population(16),\n",
    "                                toolbox,\n",
    "                                mu=8,\n",
    "                                lambda_=10,\n",
    "                                cxpb=0.3,\n",
    "                                mutpb=0.6,\n",
    "                                ngen=16,\n",
    "                                halloffame=hof,\n",
    "                                stats=stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time population, logbook = run_mu_plus_lambda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = logbook.plot(['min','avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `eaMuCommaLambdaWithExtraLog`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hof.clear()\n",
    "pmap.close()\n",
    "pmap = ConcurrentMap(10)\n",
    "toolbox.register('evaluate', evaluate)\n",
    "toolbox.register('map', pmap.map)\n",
    "def run_mu_comma_lambda():\n",
    "    return eaMuCommaLambdaWithExtraLog(toolbox.population(10),\n",
    "                                toolbox,\n",
    "                                mu=10,\n",
    "                                lambda_=20,\n",
    "                                cxpb=0.3,\n",
    "                                mutpb=0.6,\n",
    "                                ngen=16,\n",
    "                                halloffame=hof,\n",
    "                                stats=stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time population, logbook = run_mu_plus_lambda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = logbook.plot(['min','avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sklearn\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from functools import lru_cache, partial\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "class ScikitLearnerCV:\n",
    "    \"Make use of sklearn cross_val_predict interface to optimize paramters.\"\n",
    "\n",
    "    def __init__(self, learner, X, y):\n",
    "        \"\"\"Create the class instance.\n",
    "        \n",
    "        **Parameters**\n",
    "        \n",
    "        - learner: the scikit-learn estimator\n",
    "        \"\"\"\n",
    "        self.learner = learner\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getattr__(self, key):\n",
    "        return getattr(self.learner, key)\n",
    "\n",
    "    def __dir__(self):\n",
    "        return set(super().__dir__() + list(self.__dict__.keys()) +\n",
    "                   dir(self.learner))\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cv_loss_with_params(self,\n",
    "                            loss_func=None,\n",
    "                            callbacks=tuple(),\n",
    "                            groups=None,\n",
    "                            cv=None,\n",
    "                            n_jobs=None,\n",
    "                            verbose=0,\n",
    "                            pre_dispatch='2*n_jobs',\n",
    "                            method='predict',\n",
    "                            fit_params=None,\n",
    "                            **params):\n",
    "        \"\"\"Calculate loss of the estimator on validation set, and also the values of callbacks.\n",
    "\n",
    "        **Parameters**\n",
    "        \n",
    "        - loss_func: loss function to use. By default, if estimator support to predict probability, it will be\n",
    "        log_loss, otherwise it will be accuracy score.\n",
    "        - callbacks: callbacks to also evaluate. Default is empty tuple.\n",
    "        - params: parameters to use when train the estimator.\n",
    "\n",
    "        **Returns**\n",
    "        \n",
    "        Return values will be a two element tuple.\n",
    "        - the first is values of the loss function\n",
    "        - the other is values of all callbacks\n",
    "        \"\"\"\n",
    "        return self.loss_with_params(self.X,\n",
    "                                     self.y,\n",
    "                                     loss_func=loss_func,\n",
    "                                     callbacks=callbacks,\n",
    "                                     groups=groups,\n",
    "                                     cv=cv,\n",
    "                                     n_jobs=n_jobs,\n",
    "                                     verbose=verbose,\n",
    "                                     pre_dispatch=pre_dispatch,\n",
    "                                     method=method,\n",
    "                                     fit_params=fit_params,\n",
    "                                     **params)\n",
    "\n",
    "    def loss_with_params(self,\n",
    "                         X,\n",
    "                         y,\n",
    "                         loss_func=None,\n",
    "                         callbacks=tuple(),\n",
    "                         groups=None,\n",
    "                         cv=None,\n",
    "                         n_jobs=None,\n",
    "                         verbose=0,\n",
    "                         pre_dispatch='2*n_jobs',\n",
    "                         method='predict',\n",
    "                         fit_params=None,\n",
    "                         **params):\n",
    "        estimator = sklearn.clone(self.learner)\n",
    "        estimator.set_params(**params)\n",
    "        y_pred = cross_val_predict(estimator,\n",
    "                                   X,\n",
    "                                   y,\n",
    "                                   groups=groups,\n",
    "                                   cv=cv,\n",
    "                                   n_jobs=n_jobs,\n",
    "                                   verbose=verbose,\n",
    "                                   pre_dispatch=pre_dispatch,\n",
    "                                   fit_params=fit_params,\n",
    "                                   method=method)\n",
    "        if loss_func is None:\n",
    "            if method == 'predict_proba':\n",
    "                loss_func = log_loss\n",
    "            else:\n",
    "                loss_func = lambda x, y: -accuracy_score(x, y)\n",
    "        return [loss_func(y, y_pred)\n",
    "                ], [callback(y, y_pred) for callback in callbacks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ScikitLearnerCV.__init__)\n",
    "show_doc(ScikitLearnerCV.cv_loss_with_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of using `ScikitLearnerCV`\n",
    "`DigitsCV` is a sub-class of `ScikitLearnerCV`. Check `sample_cases` for more information.# Optimize parameters based on CV result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eptune.sample_cases import DigitsCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv_svc_digits = DigitsCV(SVC())\n",
    "def cv_evaluate(params):\n",
    "    return cv_svc_digits.cv_loss_with_params(callbacks=(accuracy_score, ),\n",
    "                                             cv=StratifiedKFold(n_splits=3),\n",
    "                                             **params)\n",
    "\n",
    "\n",
    "hof.clear()\n",
    "pmap.close()\n",
    "pmap = ConcurrentMap(10)\n",
    "toolbox.register('evaluate', cv_evaluate)\n",
    "toolbox.register('map', pmap.map)\n",
    "from eptune.crossover import cxDictBlenderIfCan\n",
    "toolbox.register(\"mate\", cxDictBlenderIfCan, alpha=1.2, indpb=0.5, fix_invalid=True)\n",
    "\n",
    "from eptune.mutation import mutDictRand\n",
    "toolbox.register(\"mutate\", partial(mutDictRand, params=params, indpb=0.6))\n",
    "\n",
    "\n",
    "\n",
    "def runcv():\n",
    "    return eaSimpleWithExtraLog(toolbox.population(10),\n",
    "                                toolbox,\n",
    "                                cxpb=0.5,\n",
    "                                mutpb=0.4,\n",
    "                                ngen=16,\n",
    "                                halloffame=hof,\n",
    "                                elitism=True,\n",
    "                                stats=stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time population, logbook = runcv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = logbook.plot(['min', 'avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using other algorithms to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `eaMuPlusLambdaWithExtraLog`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hof.clear()\n",
    "pmap.close()\n",
    "pmap = ConcurrentMap(10)\n",
    "from eptune.crossover import cxDictBlenderIfCan\n",
    "toolbox.register(\"mate\", cxDictBlenderIfCan, alpha=1.2, indpb=0.9, fix_invalid=True)\n",
    "\n",
    "from eptune.mutation import mutDictRand\n",
    "toolbox.register(\"mutate\", partial(mutDictRand, params=params, indpb=0.9))\n",
    "\n",
    "toolbox.register('evaluate', cv_evaluate)\n",
    "toolbox.register('map', pmap.map)\n",
    "def runcv_mu_plus_lambda():\n",
    "    return eaMuPlusLambdaWithExtraLog(toolbox.population(10),\n",
    "                                toolbox,\n",
    "                                mu=10,\n",
    "                                lambda_=10,\n",
    "                                cxpb=0.4,\n",
    "                                mutpb=0.4,\n",
    "                                ngen=16,\n",
    "                                halloffame=hof,\n",
    "                                stats=stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time population, logbook = runcv_mu_plus_lambda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = logbook.plot(['min', 'avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `eaMuCommaLambdaWithExtraLog`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hof.clear()\n",
    "pmap.close()\n",
    "pmap = ConcurrentMap(10)\n",
    "toolbox.register('evaluate', cv_evaluate)\n",
    "toolbox.register('map', pmap.map)\n",
    "def runcv_mu_comma_lambda():\n",
    "    return eaMuCommaLambdaWithExtraLog(toolbox.population(10),\n",
    "                                toolbox,\n",
    "                                mu=10,\n",
    "                                lambda_=10,\n",
    "                                cxpb=0.3,\n",
    "                                mutpb=0.5,\n",
    "                                ngen=16,\n",
    "                                halloffame=hof,\n",
    "                                stats=stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time population, logbook = runcv_mu_comma_lambda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = logbook.plot(['min', 'avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the content in HallofFame at anytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,i.extra) for i in hof.items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
